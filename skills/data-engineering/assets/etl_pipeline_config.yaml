# ETL Pipeline Configuration
pipeline:
  name: data-ingestion
  schedule: "0 */6 * * *"

sources:
  - name: postgres_source
    type: postgresql
    connection: "${POSTGRES_URL}"
    tables: [users, orders, products]

  - name: api_source
    type: rest_api
    url: "https://api.example.com/data"
    auth: bearer_token
    pagination: cursor

transformations:
  - name: clean_data
    type: sql
    query: "SELECT * FROM source WHERE valid = true"

  - name: aggregate
    type: spark
    script: aggregate_metrics.py

destinations:
  - name: data_warehouse
    type: redshift
    table: analytics.fact_sales
    mode: upsert
    key: id

monitoring:
  alerts:
    - on_failure: slack
    - on_delay: email
  metrics:
    - rows_processed
    - execution_time
